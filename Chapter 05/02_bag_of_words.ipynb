{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    return word_tokenize(document)\n",
    "\n",
    "def is_no_punctuation(word):\n",
    "    return word.isalnum()\n",
    "\n",
    "def is_no_stopword(word):\n",
    "    return word.lower() not in set(stopwords.words('english'))\n",
    "\n",
    "def stem(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def categorize(words):\n",
    "    tags = nltk.pos_tag(words)\n",
    "    return [tag for word, tag in tags]\n",
    "\n",
    "def lemmatize(words, tags):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    pos = [tag_dict.get(t[0].upper(), wordnet.NOUN) for t in tags]\n",
    "    return [lemmatizer.lemmatize(w, pos=p) for w, p in zip(words, pos)]\n",
    "\n",
    "def count(words):\n",
    "    return Counter(words)\n",
    "\n",
    "def sklearn_count_vect(data, **kwargs):\n",
    "    count_vect = CountVectorizer(**kwargs)\n",
    "    X_train_counts = count_vect.fit_transform(data)\n",
    "    print(X_train_counts)\n",
    "    print(count_vect.vocabulary_)\n",
    "    return X_train_counts\n",
    "\n",
    "def sklearn_tfidf_vect(data, **kwargs):\n",
    "    vec = TfidfVectorizer(**kwargs)\n",
    "    X_train_counts = vec.fit_transform(data)\n",
    "    print(X_train_counts)\n",
    "    print(vec.get_feature_names())\n",
    "    return X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Almost before we knew it, we had left the ground. The unknown holds its grounds.\n",
      "Tokenized: ['Almost', 'before', 'we', 'knew', 'it', ',', 'we', 'had', 'left', 'the', 'ground', '.', 'The', 'unknown', 'holds', 'its', 'grounds', '.']\n",
      "Stripped stopwords and punctuation: ['Almost', 'knew', 'left', 'ground', 'unknown', 'holds', 'grounds']\n",
      "Stemming: ['almost', 'knew', 'left', 'ground', 'unknown', 'hold', 'ground']\n",
      "Word categories: ['RB', 'VBD', 'VBN', 'NN', 'JJ', 'VBZ', 'NNS']\n",
      "Lemmatization: ['almost', 'know', 'leave', 'ground', 'unknown', 'hold', 'ground']\n",
      "Bag of words: Counter({'ground': 2, 'almost': 1, 'know': 1, 'leave': 1, 'unknown': 1, 'hold': 1})\n",
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 8)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "{'almost': 0, 'before': 1, 'we': 12, 'knew': 8, 'it': 6, 'had': 4, 'left': 9, 'the': 10, 'ground': 2, 'unknown': 11, 'holds': 5, 'its': 7, 'grounds': 3}\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "{'almost': 0, 'know': 3, 'leave': 4, 'ground': 1, 'unknown': 5, 'hold': 2}\n",
      "  (0, 2)\t0.3333333333333333\n",
      "  (0, 5)\t0.3333333333333333\n",
      "  (0, 1)\t0.6666666666666666\n",
      "  (0, 4)\t0.3333333333333333\n",
      "  (0, 3)\t0.3333333333333333\n",
      "  (0, 0)\t0.3333333333333333\n",
      "['almost', 'ground', 'hold', 'know', 'leave', 'unknown']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"Almost before we knew it, we had left the ground. The unknown holds its grounds.\"\n",
    "print('Document:', document)\n",
    "\n",
    "tokens = tokenize(document)\n",
    "print('Tokenized:', tokens)\n",
    "\n",
    "words = [w for w in tokens if is_no_punctuation(w) and is_no_stopword(w)]\n",
    "print('Stripped stopwords and punctuation:', words)\n",
    "\n",
    "words = stem(words)\n",
    "print('Stemming:', words)\n",
    "\n",
    "tokens_pos = categorize(tokens)\n",
    "words_pos = [pos for w, pos in zip(tokens, tokens_pos) if is_no_punctuation(w) and is_no_stopword(w)]\n",
    "print('Word categories:', words_pos)\n",
    "\n",
    "words = lemmatize(words, words_pos)\n",
    "print('Lemmatization:', words)\n",
    "\n",
    "bag_of_words = count(words)\n",
    "print('Bag of words:', bag_of_words)\n",
    "\n",
    "sklearn_count_vect([document])\n",
    "sklearn_count_vect([\" \".join(words)])\n",
    "sklearn_tfidf_vect([\" \".join(words)])\n",
    "#sklearn_count_vect([\" \".join(words)], ngram_range=(2,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
